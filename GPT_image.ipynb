{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsNXmq4zp8bZ"
      },
      "source": [
        "## 1. BLIP으로 이미지를 설명하는 프롬프트 1차 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u3hHqndo16Ze"
      },
      "outputs": [],
      "source": [
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def BLIP_caption(image_path):\n",
        "  # 모델 로드\n",
        "  processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "  model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "  # 이미지로부터 설명 생성\n",
        "  inputs = processor(images=Image.open(image_path), return_tensors=\"pt\")\n",
        "\n",
        "  # 프롬프트 생성\n",
        "  out = model.generate(**inputs)\n",
        "  caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "  return caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDvyskTjqKwA",
        "outputId": "ace8f83e-052a-45b6-8ce3-ff12a368ff42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a woman taking a picture of herself\n"
          ]
        }
      ],
      "source": [
        "# 이미지 열기\n",
        "image_path = \"./example.png\"\n",
        "caption = BLIP_caption(image_path)\n",
        "print(caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OtcZ06XqdRt"
      },
      "source": [
        "## 2. GPT를 이용하여 이미지의 프롬프트 구체화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8OmlJNExqheI"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import requests\n",
        "\n",
        "OPENAI_KEY = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "K7R_yZqAosM8"
      },
      "outputs": [],
      "source": [
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "def get_gpt_response(image_filename, question):\n",
        "    # Path to your image\n",
        "    image_path = image_filename\n",
        "\n",
        "    # Getting the base64 string\n",
        "    base64_image = encode_image(image_path)\n",
        "\n",
        "    headers = {\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Authorization\": f\"Bearer {OPENAI_KEY}\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "      \"model\": \"gpt-4o\",\n",
        "      \"messages\": [\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": [\n",
        "            {\n",
        "              \"type\": \"text\",\n",
        "              \"text\": question\n",
        "            },\n",
        "            {\n",
        "              \"type\": \"image_url\",\n",
        "              \"image_url\": {\n",
        "                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "              }\n",
        "            }\n",
        "          ]\n",
        "        }\n",
        "      ],\n",
        "      \"max_tokens\": 1024\n",
        "    }\n",
        "\n",
        "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "\n",
        "    return response.json()['choices'][0]['message']['content']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62iIOZKZqlH4"
      },
      "outputs": [],
      "source": [
        "prompt_background = get_gpt_response(image_path, 'Generate the detailed prompt that represents the background of given image.(Write only about the background, not the person/thing or style.)\\n'+'Please refer to the following prompt.: \\n'+caption)\n",
        "\n",
        "prompt_style = get_gpt_response(image_path, 'Generate the detailed prompt that represents the picture style of given image.(Write only about the style, not the background or person/thing.)\\n'+'Please refer to the following prompt.: \\n'+caption)\n",
        "\n",
        "prompt_personthing = get_gpt_response(image_path, 'Generate the detailed prompt that represents the person/thing of given image.(Write only about the person/thing, not the background or style.)\\n'+'Please refer to the following prompt.: \\n'+caption)\n",
        "\n",
        "improved_prompt = f\"{prompt_background}\\n{prompt_style}\\n{prompt_personthing}\"\n",
        "\n",
        "print(improved_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfEAhxLMrMhT"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=OPENAI_KEY)\n",
        "\n",
        "response = client.images.generate(\n",
        "  model=\"dall-e-3\",\n",
        "  prompt=improved_prompt,\n",
        "  size=\"1024x1024\",\n",
        "  quality=\"standard\",\n",
        "  n=1,\n",
        ")\n",
        "\n",
        "image_url = response.data[0].url\n",
        "\n",
        "im = Image.open(requests.get(image_url, stream=True).raw)\n",
        "im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EawX9HI8u9PB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}